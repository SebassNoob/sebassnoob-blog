The world is changing. As AI has exploded onto the scene within the past 2 years, the ethics of artificial intelligence has come to the forefront of the scientific media. Questions like ‘Should robots have rights?’ and ‘Should we pause research into AI in case of a robot takeover?’ seem ever so pertinent in everyone’s mind. As of the time of writing, OpenAI has released a ‘revolutionary’, or so it seems, large language model: [GPT-4o](https://openai.com/index/hello-gpt-4o/). It is their new flagship model, being able to parse a wide range of visual, auditory and textual stimuli in real time, and do so in almost real time. Preliminary reactions by the media have been, expectedly, astounded by this ‘new frontier’ and their tech demo was quite impressive to me. But of course, with AI seemingly becoming ever more human in terms of thinking ability, the big question is, where do we draw the boundary of becoming human? When should we be giving AI models rights?

The answer is not simple. There are a variety of factors that we must consider, one of which must definitely be AI’s human values. Firms developing these models have the moral and legal responsibility to align them to common values aimed at progress for the good of all mankind. We can probably agree that AI should be moral, making decisions that are principled and ethical; they should be unbiased, so as to not be in support of radical politics; and most of all they should be humane – in terms of not negatively affecting human life. I acknowledge that these are very loose definitions, and even the criteria for ‘human values’ are subject to debate. But the fact still stands, whatever standards the world ends up adopting, AI models must conform with human values, lest we risk the terrifying reality of a machine intelligence powered takeover.

[AI hallucinations](https://www.ibm.com/topics/ai-hallucinations) must also be addressed. At the moment, our AI assistants are prone to blatantly make up speculative, fantastical new ideas and somehow phrase them as fact. Air Canada learned that the hard way. In a brand new case of its league, a passenger who was grieving at the time, having lost his grandmother, clarified the Air Canada’s pricing rules regarding their bereavement policy with their chatbot. The hallucinating chatbot then provided incorrect information that did not align with the airline’s policy. Following which, the passenger encountered issues with his ticketing and subsequently sued Air Canada, winning the case in a setback to chatbots on the internet. Perhaps companies have moved too quickly into the AI sphere. As the ‘hot’ new trend of the internet, firms utilise error-prone AI models and weave them into all levels of their workflow, from integrating it into their websites or using AI-powered autocomplete for writing emails or code – it is a disaster waiting to happen. Someday, someone will, with the help of AI, write or say something that will cost them dearly as with what happened with AIr Canada.

There’s no silver bullet to resolve the AI troubles and ethical issues that we have immediately. It is a long arduous road ahead of us, marked by legal, social and moral issues along the way. It is up to the leaders of our time to make decisions in the interest for all of society, and perhaps someday we will have AIs being elevated to personhood.
